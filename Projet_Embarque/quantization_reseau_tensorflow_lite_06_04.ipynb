{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction à la quantization \n",
        "\n",
        "Laurent cetinsoy\n",
        "\n",
        "Les réseaux de neurones prennent beaucoup de place et il peut être difficile de les faire rentrer sur certains dispositifs embarqués. \n",
        "\n",
        "Il existe plusieurs méthodes pour réduire la taille et augmenter la vitesse d'executer des réseaux de neurone. Par exemple il y a ce qu'on appelle la quantization et le pruning.\n",
        "\n",
        "Dans ce notebook on va faire une introduction à la quantization avec la librairie tensorflow lite.\n",
        "\n",
        "\n",
        "## Quantization post training\n",
        "\n",
        "Dans un premier temps on va quantifier notre réseau après l'avoir entraîné normalement. \n",
        "\n",
        "\n",
        "Entraîner un réseau de neurone convolutionnel simple avec keras pour faire de la classification MNIST (ou un autre dataset simple de votre choix si (vous en avez marre de ce dataset - https://keras.io/api/datasets/)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "efc9100a06c04d0b8b17c557d7cbbd41",
        "deepnote_cell_type": "markdown",
        "id": "VhtssgLNL6dc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size = (3, 3), activation='relu'))\n",
        "model.add(Conv2D(64, kernel_size = (3, 3), activation='relu'))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(200, activation='relu')) \n",
        "model.add(Dense(10, activation='softmax'))#nombre de classes\n",
        "\n",
        "model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"sgd\")\n",
        "\n",
        "X_train = X_train.reshape(60000, 28, 28, 1) / 255 #MinMaxScaling en divisant par le max parce que le min est égal à 0\n",
        "\n",
        "model.fit(X_train,y_train)"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "87bc3d9f649e416983c0ee9b59bd2de6",
        "deepnote_cell_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLVqD4cQL6dd",
        "outputId": "7f7326c3-ddff-49ce-9da3-753527a10b95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "1875/1875 [==============================] - 21s 4ms/step - loss: 0.3015\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0f53f66370>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": [
        "Afficher le nombre de paramètre du modèle"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "b984ac621a2249d1b955535f9f9ab55b",
        "deepnote_cell_type": "markdown",
        "id": "ifqWCikpL6de"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.summary())"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "11935acab177492cbcd4a8aafea0bdf0",
        "deepnote_cell_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSigFLrIL6de",
        "outputId": "9bd48586-2439-4d3d-8765-03588b0ed110"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (32, 26, 26, 32)          320       \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (32, 24, 24, 64)          18496     \n",
            "                                                                 \n",
            " flatten (Flatten)           (32, 36864)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (32, 200)                 7373000   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (32, 10)                  2010      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,393,826\n",
            "Trainable params: 7,393,826\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sauvegarder votre modèle et afficher la taille du fichier. Si on applique une bête règle de trois, quelle est la taille occupée par paramètre ? "
      ],
      "metadata": {
        "tags": [],
        "cell_id": "4782a0f186d14d4e91cfd903998f8710",
        "deepnote_cell_type": "markdown",
        "id": "yjAWl7S_L6de"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import os\n",
        "\n",
        "model_file_name = \"mnist_.joblib\"\n",
        "joblib.dump(model, model_file_name)\n",
        "\n",
        "print(\"taille du fichier :\", os.path.getsize('/content/mnist_.joblib'), \"octets\")\n",
        "print(\"taille par paramètre : 59177771 / 7393826 = \", 59177771  / 7393826, \"octets\")"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "8f50426e286c4cf39610c41ed5cdfeb7",
        "deepnote_cell_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODrTll-hL6df",
        "outputId": "03e415bb-8090-4356-901d-e74a94512d71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "taille du fichier : 59177443 octets\n",
            "taille par paramètre : 59177771 / 7393826 =  8.003673740766958 octets\n"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "source": [
        "On va maintenant convertir notre modèle keras en modèle tensorflow lite. \n",
        "\n",
        "Installer la librairie tensorflow lite créer une instance de la class TFLiteConverter à partir de votre modèle keras\n"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "2e0f0d5c02dc40db9fae96aa67d4de06",
        "deepnote_cell_type": "markdown",
        "id": "BsdxOOnFL6df"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "converter = tensorflow.lite.TFLiteConverter.from_keras_model(model)"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "8c1f6d62ff6a416aa3ec3b05ebccddb9",
        "deepnote_cell_type": "code",
        "id": "VNTAhDPqL6df"
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convertir votre modèle et le sauvegarder dans un fichier nommé model.tflite. Sa taille est-elle plus petite ? "
      ],
      "metadata": {
        "tags": [],
        "cell_id": "13a2cc6cc09f4e799d4e8641521ece8e",
        "deepnote_cell_type": "markdown",
        "id": "2kVPAhtvL6df"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.tflite = converter.convert()\n",
        "\n",
        "model_tflitefile_name = \"mnistLite_.joblib\"\n",
        "joblib.dump(model.tflite, model_tflitefile_name)\n",
        "\n",
        "print(\"taille du fichier lite :\", os.path.getsize('/content/mnistLite_.joblib'), \"octets\")\n",
        "print(\"Le fichier est 2 fois moins gros\")"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "2ca07a9b6ee74a6594c3a33313decfc0",
        "deepnote_cell_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ed8aPp4wL6df",
        "outputId": "370ed27f-022f-434f-cf26-c90822da753b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "taille du fichier lite : 29578297 octets\n",
            "Le fichier est 2 fois moins gros\n"
          ]
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "markdown",
      "source": [
        "On va maintenant spécifier des optimisations au converter. \n",
        "\n",
        "1. Recréer un converter\n",
        "\n",
        "2. modifier son attribut optimizations pour ajouter une liste d'optimisation avec la valeur tf.lite.Optimize.DEFAULT\n",
        "\n",
        "3. Relancer la conversion du modèle, sauvegarder le modèle et regarder la taille du fichier généré"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "0e707074068f44ffad07b9301948782b",
        "deepnote_cell_type": "markdown",
        "id": "nXPBJI-FL6df"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimized_converter = tensorflow.lite.TFLiteConverter.from_keras_model(model)\n",
        "optimized_converter.optimizations = [tensorflow.lite.Optimize.DEFAULT]\n",
        "\n",
        "optimized_model = optimized_converter.convert()\n",
        "model_optimized_name = \"model_optimize.joblib\"\n",
        "joblib.dump(optimized_model, model_optimized_name)\n",
        "\n",
        "print(\"taille du fichier lite :\", os.path.getsize('/content/model_optimize.joblib'), \"octets\")\n",
        "print(\"Le fichier est très largement plus léger\")"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "42b10a79262949aeadb6f2f82eae6f58",
        "deepnote_cell_type": "code",
        "id": "1aMP8kbcL6dg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01a67be4-d6c2-4156-9a6b-65149edb4091"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "taille du fichier lite : 7399513 octets\n",
            "Le fichier est très largement plus léger\n"
          ]
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quelle type  de quantization Optimize.Default, utilise-t-elle ?\n"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "e083971b3c7647818e1532ed9c7edb41",
        "deepnote_cell_type": "markdown",
        "id": "5dpsXb7yL6dg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "tags": [],
        "cell_id": "c4952d60d77243439193b6e9b4d573e9",
        "deepnote_cell_type": "markdown",
        "id": "EM975ITnL6dg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantization aware training \n",
        "\n",
        "Dans cette section on va s'intéresser à l'entraînement sensible à la quantification. L'idée est de simuler les effets de la quantification pendant l'entraînement pour que le modèle ajuste les poids afin de tenir ocmpte de la quantification. \n",
        "\n",
        "Reprendre le modèle entraîné sur MNIST\n"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "0bfeb7a588454ae6942059d7d79be609",
        "deepnote_cell_type": "markdown",
        "id": "0KUjX42wL6dg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = joblib.load('/content/mnist_.joblib')"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "11d35b87030b49eb99c77c4a6fe47db3",
        "deepnote_cell_type": "code",
        "id": "hNAoEWC_L6dg"
      },
      "outputs": [],
      "execution_count": 7
    },
    {
      "cell_type": "markdown",
      "source": [
        "A l'aide de la fonction quantize de tensorflow_model_optimization, créer une seconde version de votre modèle entraîné nommé qat_model"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "af90aac8bbb94f86b728031f5ae0a66f",
        "deepnote_cell_type": "markdown",
        "id": "7_StB5riL6dg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compiler le modèle"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "868beffe9e8740d98cbfd8458af4117a",
        "deepnote_cell_type": "markdown",
        "id": "LX0G4qPvL6dh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Afficher le summury du modèle. D'après vous ce modèle est-il quantifié ? "
      ],
      "metadata": {
        "tags": [],
        "cell_id": "4dddea973f404fb699b134a67d1a2dc5",
        "deepnote_cell_type": "markdown",
        "id": "tm6kEB8mL6dh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Réentraîner votre modèle sur un sous ensemble des modèles sur une ou deux epochs et afficher la performance sur le train et test set"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "7b3a873512174205b6d6f4aacd7480c8",
        "deepnote_cell_type": "markdown",
        "id": "uzEMH593L6dh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convertir votre modèle avec TFLite"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "cc4f49a0e7894b8385e90c2330a55bb1",
        "deepnote_cell_type": "markdown",
        "id": "Uq-Ijq4DL6dh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tensorflow-model-optimization\n",
        "import tensorflow_model_optimization as tfmot"
      ],
      "metadata": {
        "id": "iQsry984pKev",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dd8378d-b332-4063-ebdf-3b921aac4225"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.6/240.6 KB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.24.2 which is incompatible.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.24.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qat_model = tfmot.quantization.keras.quantize_model(loaded_model)\n",
        "\n",
        "qat_model.compile(loss = \"sparse_categorical_crossentropy\", optimizer = \"sgd\")\n",
        "\n",
        "print(qat_model.summary())"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "68918464bfb14b749d3848fe25b5e02e",
        "deepnote_cell_type": "code",
        "id": "8mVwTK-YL6di",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1419018b-963e-4947-e445-88b7408d4316"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " quantize_layer (QuantizeLay  (32, 28, 28, 1)          3         \n",
            " er)                                                             \n",
            "                                                                 \n",
            " quant_conv2d (QuantizeWrapp  (32, 26, 26, 32)         387       \n",
            " erV2)                                                           \n",
            "                                                                 \n",
            " quant_conv2d_1 (QuantizeWra  (32, 24, 24, 64)         18627     \n",
            " pperV2)                                                         \n",
            "                                                                 \n",
            " quant_flatten (QuantizeWrap  (32, 36864)              1         \n",
            " perV2)                                                          \n",
            "                                                                 \n",
            " quant_dense (QuantizeWrappe  (32, 200)                7373005   \n",
            " rV2)                                                            \n",
            "                                                                 \n",
            " quant_dense_1 (QuantizeWrap  (32, 10)                 2015      \n",
            " perV2)                                                          \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,394,038\n",
            "Trainable params: 7,393,826\n",
            "Non-trainable params: 212\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "execution_count": 9
    },
    {
      "cell_type": "code",
      "source": [
        "qat_model.fit(X_train,y_train,epochs=2,validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFVo9Uo5X9D8",
        "outputId": "013701ea-620d-4be5-e487-17b14bcf4e29"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "1875/1875 [==============================] - 26s 13ms/step - loss: 0.1367 - val_loss: 0.1379\n",
            "Epoch 2/2\n",
            "1875/1875 [==============================] - 18s 9ms/step - loss: 0.1007 - val_loss: 0.1380\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0f401990d0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Evaluate on test data\")\n",
        "results = qat_model.evaluate(X_test, y_test)\n",
        "print(\"test loss, test acc:\", results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sK-6XkzcFdz",
        "outputId": "1358f5f6-3ac8-4453-985b-5f8dc5605599"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluate on test data\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.1380\n",
            "test loss, test acc: 0.13801495730876923\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qat_converter = tensorflow.lite.TFLiteConverter.from_keras_model(qat_model)\n",
        "tflite_qat_model = qat_converter.convert()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ir-aXy8c61g",
        "outputId": "78191af2-ebf8-4026-8bdc-222dc7fcabbf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as conv2d_layer_call_fn, conv2d_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op, conv2d_1_layer_call_fn, conv2d_1_layer_call_and_return_conditional_losses while saving (showing 5 of 12). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparer la performance du modèle Quantified aware training, au modèle original et au modèle quantifié post training"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "d80e6fbfea9646919a9cbe2c8fa41b2d",
        "deepnote_cell_type": "markdown",
        "id": "oAPLX2KQL6di"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "tags": [],
        "cell_id": "bdd43405f5fd46ce892d957bb43a353a",
        "deepnote_cell_type": "code",
        "id": "9Sck2uslL6di"
      },
      "outputs": [],
      "execution_count": 12
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sauvegarder le modèle QAT et comparer les tailles des modèles"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "6472c7a129394639838f76381e562588",
        "deepnote_cell_type": "markdown",
        "id": "GoZVeWr5L6di"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(tflite_qat_model, \"QAT_model.joblib\")\n",
        "\n",
        "print(\"taille du fichier :\", os.path.getsize('/content/QAT_model.joblib'), \"octets\")"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "d3c2951a1fd14be3b5e3c85e05a056ea",
        "deepnote_cell_type": "code",
        "id": "-cwLP-k-L6di",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31847bce-0aa4-4bc4-be37-a051eab18b19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "taille du fichier : 29584201 octets\n"
          ]
        }
      ],
      "execution_count": 13
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bonus : déployer votre modèle sur votre téléphone ou un dispositif embarqué si vous en disposez d'un. "
      ],
      "metadata": {
        "tags": [],
        "cell_id": "4a006a6d1a194e418072192da1a920de",
        "deepnote_cell_type": "markdown",
        "id": "J_H_WcG8L6di"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "tags": [],
        "cell_id": "927af747794641fa936ad727c6bf75d0",
        "deepnote_cell_type": "code",
        "id": "YtKw-lYeL6di"
      },
      "outputs": [],
      "execution_count": 12
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bonus : Obtenir un modèle qui sera à la fois quantifié et élagué (prunned) en s'aidant de la documentation (https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras)"
      ],
      "metadata": {
        "tags": [],
        "cell_id": "e7635cb278ab43589a851e2c2de0d8ef",
        "deepnote_cell_type": "markdown",
        "id": "5utZlrEEL6di"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "tags": [],
        "cell_id": "a1eacb94d0aa4dbd872c8babacf8bb8d",
        "deepnote_cell_type": "code",
        "id": "aKcjfPvcL6di"
      },
      "outputs": [],
      "execution_count": 12
    },
    {
      "cell_type": "markdown",
      "source": [
        "A l'aide de tensorflow lite / tensorflow lite micro "
      ],
      "metadata": {
        "tags": [],
        "cell_id": "71e07172d7514f8f84e744177e1bfb2b",
        "is_collapsed": false,
        "formattedRanges": [],
        "deepnote_cell_type": "text-cell-p",
        "id": "lRymkQTkL6dj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=0d51e245-899d-41d6-b23b-cf3e4bbbc6ea' target=\"_blank\">\n",
        "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
        "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
      ],
      "metadata": {
        "tags": [],
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown",
        "id": "FKA_8Ll-L6dj"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "deepnote": {
      "is_reactive": false
    },
    "orig_nbformat": 2,
    "deepnote_notebook_id": "fb1d23f975ba410e92fed9f5b8cbb7e6",
    "deepnote_execution_queue": [],
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  }
}